<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Object Detection – Full Screen</title>

  <!-- TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0"></script>

  <!-- COCO-SSD -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

  <style>
    html, body {
      margin: 0;
      padding: 0;
      background: black;
      overflow: hidden;
      height: 100%;
      width: 100%;
    }

    #container {
      position: relative;
      width: 100vw;
      height: 100vh;
    }

    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
    }

    #status {
      position: absolute;
      bottom: 10px;
      left: 50%;
      transform: translateX(-50%);
      color: white;
      background: rgba(0,0,0,0.6);
      padding: 6px 12px;
      border-radius: 10px;
      font-size: 14px;
      font-family: Arial, sans-serif;
      z-index: 10;
    }
  </style>
</head>

<body>
  <div id="container">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
    <div id="status">Loading model…</div>
  </div>

<script>
  const video = document.getElementById("video");
  const canvas = document.getElementById("canvas");
  const ctx = canvas.getContext("2d");
  const status = document.getElementById("status");

  let model;
  let lastSpokenObject = "";

  async function setupCamera() {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: "environment" },
      audio: false
    });
    video.srcObject = stream;
    return new Promise(resolve => {
      video.onloadedmetadata = () => resolve();
    });
  }

  function resizeCanvas() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
  }

  function speakObject(objectName) {
    if (objectName !== lastSpokenObject) {
      const utterance = new SpeechSynthesisUtterance(objectName);
      utterance.rate = 1;
      utterance.pitch = 1;
      speechSynthesis.cancel();
      speechSynthesis.speak(utterance);
      lastSpokenObject = objectName;
    }
  }

  function draw(predictions) {
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    predictions
      .filter(p => p.score > 0.5)
      .forEach(p => {
        const [x, y, w, h] = p.bbox;

        // Limit box to object size
        const boxWidth = Math.min(w, canvas.width - x);
        const boxHeight = Math.min(h, canvas.height - y);

        ctx.strokeStyle = "#00ff00";
        ctx.lineWidth = 2;
        ctx.strokeRect(x, y, boxWidth, boxHeight);

        ctx.fillStyle = "#00ff00";
        ctx.font = "18px Arial";
        ctx.fillText(p.class, x, y > 20 ? y - 5 : y + 20);

        speakObject(p.class);
      });
  }

  async function detectLoop() {
    resizeCanvas();
    const predictions = await model.detect(video);
    draw(predictions);
    requestAnimationFrame(detectLoop);
  }

  async function start() {
    await setupCamera();
    status.innerText = "Loading object detection model…";
    model = await cocoSsd.load();
    status.innerText = "Detecting objects";
    detectLoop();
  }

  start();
</script>
</body>
</html>


